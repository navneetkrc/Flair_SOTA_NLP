{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flair_text_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/Flair_SOTA_NLP/blob/master/Flair_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CRI11WBajJnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Dataset\n",
        "We’ll be working on the [AV Twitter Sentiment Analysis](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)  practice problem. Go ahead and download the dataset from there (you’ll need to register/log in first).\n",
        "\n",
        "The problem statement posed by this challenge is:\n",
        "\n",
        "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n"
      ]
    },
    {
      "metadata": {
        "id": "zXkMo_cHjeiy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Steps Involved\n",
        "**1. Text Classification Using Flair Embeddings**\n",
        "\n",
        "Overview of steps:\n",
        "\n",
        "Step 1: Import the data into the local Environment of Colab:\n",
        "\n",
        "Step 2: Installing Flair\n",
        "\n",
        "Step 3: Preparing text to work with Flair\n",
        "\n",
        "Step 4: Word Embeddings with Flair\n",
        "\n",
        "Step 5: Vectorizing the text\n",
        "\n",
        "Step 6: Partitioning the data for Train and Test Sets\n",
        "\n",
        "Step 7: Time for predictions!"
      ]
    },
    {
      "metadata": {
        "id": "UofJ-GS2jxGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import the data into the local Environment of Colab:"
      ]
    },
    {
      "metadata": {
        "id": "0RkGbxD2jwAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b60ce8c6-4335-41e0-c8ed-8b44af142f91"
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1GhyH4k9C4uPRnMAMKhJYOqa-V9Tqt4q8' ### File ID ###\n",
        "data = drive.CreateFile({'id': file_id})\n",
        "#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 17.5MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.6MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.0MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.4MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 2.8MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.1MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.5MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.7MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.7MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 4.1MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 4.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 7.7MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 7.7MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 7.7MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 7.6MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 7.6MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 7.6MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 39.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 8.6MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 8.6MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.6MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 8.6MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 8.6MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.4MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 8.5MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 8.6MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 8.6MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 8.7MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 45.8MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 45.1MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 47.4MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 43.6MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 43.4MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 48.7MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 49.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 49.5MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 49.3MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 48.3MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 47.5MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 48.7MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 10.6MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 10.7MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 10.6MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 10.6MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 10.6MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 10.5MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 10.5MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 10.4MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 10.4MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 10.4MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 44.5MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 48.1MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 47.4MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 45.9MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 45.4MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 47.0MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 47.6MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 51.5MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 51.3MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 49.9MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 50.1MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 49.7MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 42.5MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 43.6MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 44.6MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 45.1MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 45.3MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 45.3MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 44.2MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 12.5MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 12.5MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 12.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 13.2MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 13.1MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 13.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 12.9MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 12.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 12.8MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 12.9MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 50.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 49.3MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 47.1MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 46.0MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 47.1MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 49.3MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 49.6MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 51.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 51.4MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 50.2MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 50.9MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 51.8MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 56.9MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 58.8MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 57.9MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.6MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxhMmSS-kA9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7a2520c0-ee55-4edc-ae18-de475c0be39b"
      },
      "cell_type": "code",
      "source": [
        "#Import Dataset in Colab Notebook\n",
        "import io\n",
        "import pandas as pd\n",
        "data = pd.read_csv(io.StringIO(data.GetContentString())) \n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>user  user thanks for  lyft credit i can t us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>factsguide  society now     motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  label                                              tweet\n",
              "0           0    0.0    user when a father is dysfunctional and is s...\n",
              "1           1    0.0   user  user thanks for  lyft credit i can t us...\n",
              "2           2    0.0                                bihday your majesty\n",
              "3           3    0.0   model   i love u take with u all the time in ...\n",
              "4           4    0.0             factsguide  society now     motivation"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "E6JL5aW5ktsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 2 Download Flair Library"
      ]
    },
    {
      "metadata": {
        "id": "dA8H7XH7kkGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download flair library #\n",
        "import torch\n",
        "!pip install flair\n",
        "import flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmEKS0I8k0Ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c47e6ecc-fabd-4297-894b-3c34dc1e0edd"
      },
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "# create a sentence #\n",
        "sentence = Sentence('Blogs of Analytics Vidhya are Awesome.')\n",
        "# print the sentence to see what’s in it. #\n",
        "print(sentence)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"Blogs of Analytics Vidhya are Awesome.\" - 6 Tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nlJTvhIEk9K4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 3: Preparing text to work with Flair\n"
      ]
    },
    {
      "metadata": {
        "id": "SIacRL7Dk6lZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d39825ef-2445-429e-e25f-c31792ecb0c2"
      },
      "cell_type": "code",
      "source": [
        "#extracting the tweet part#\n",
        "text = data['tweet'] \n",
        " ## txt is a list of tweets ##\n",
        "txt = text.tolist()\n",
        "print(txt[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['  user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction     run', ' user  user thanks for  lyft credit i can t use cause they don t offer wheelchair vans in pdx      disapointed  getthanked', '  bihday your majesty', ' model   i love u take with u all the time in ur                                      ', ' factsguide  society now     motivation', '      huge fan fare and big talking before they leave  chaos and pay disputes when they get there   allshowandnogo  ', '  user camping tomorrow  user  user  user  user  user  user  user danny   ', 'the next school year is the year for exams      can t think about that       school  exams    hate  imagine  actorslife  revolutionschool  girl', 'we won    love the land     allin  cavs  champions  cleveland  clevelandcavaliers      ', '  user  user welcome here    i m   it s so  gr    ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bqqE8Ng5lRRE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 4: Word Embeddings with Flair"
      ]
    },
    {
      "metadata": {
        "id": "WxIvTYPHlFzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6f40d4e6-fd77-42f1-8bb6-0e834982d1f5"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Importing the Embeddings ##\n",
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.embeddings import CharacterEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "from flair.embeddings import BertEmbeddings\n",
        "from flair.embeddings import ELMoEmbeddings\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "### Initialising embeddings (un-comment to use others) ###\n",
        "#glove_embedding = WordEmbeddings('glove')\n",
        "#character_embeddings = CharacterEmbeddings()\n",
        "flair_forward  = FlairEmbeddings('news-forward-fast')\n",
        "flair_backward = FlairEmbeddings('news-backward-fast')\n",
        "#bert_embedding = BertEmbedding()\n",
        "#elmo_embedding = ElmoEmbedding()\n",
        "\n",
        "stacked_embeddings = StackedEmbeddings( embeddings = [ \n",
        "                                                       flair_forward, \n",
        "                                                       flair_backward\n",
        "                                                      ])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-02-26 12:05:45,778 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt not found in cache, downloading to /tmp/tmp1h4xt3u_\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19689779/19689779 [00:03<00:00, 5290312.39B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-02-26 12:05:50,653 copying /tmp/tmp1h4xt3u_ to cache at /root/.flair/embeddings/lm-news-english-forward-1024-v0.2rc.pt\n",
            "2019-02-26 12:05:50,682 removing temp file /tmp/tmp1h4xt3u_\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-02-26 12:05:57,329 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt not found in cache, downloading to /tmp/tmpb8dt6243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19689779/19689779 [00:03<00:00, 5151116.44B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-02-26 12:06:02,315 copying /tmp/tmpb8dt6243 to cache at /root/.flair/embeddings/lm-news-english-backward-1024-v0.2rc.pt\n",
            "2019-02-26 12:06:02,341 removing temp file /tmp/tmpb8dt6243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ILdEvY83lh7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "166f8110-e822-4242-f87f-35ee53b1b5da"
      },
      "cell_type": "code",
      "source": [
        "#Testing the stacked embeddings:\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('Blogs of Analytics Vidhya are Awesome.')\n",
        "# embed words in sentence #\n",
        "stacked_embeddings.embed(sentence)\n",
        "\n",
        "for token in sentence:\n",
        "  print(token.embedding)\n",
        "# data type and size of embedding #\n",
        "print(type(token.embedding))\n",
        "# storing size (length) #\n",
        "z = token.embedding.size()[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 3.0279e-04, -1.4077e-07,  2.6455e-06,  ..., -1.1807e-07,\n",
            "        -4.5203e-06,  3.4654e-03])\n",
            "tensor([-7.3398e-03, -4.8201e-05,  1.2195e-07,  ..., -1.3866e-08,\n",
            "        -1.9298e-04,  5.3008e-03])\n",
            "tensor([ 2.1015e-03, -5.1521e-06,  9.0945e-08,  ..., -3.9210e-09,\n",
            "         1.5152e-05,  1.3080e-02])\n",
            "tensor([-3.6214e-03, -1.4667e-06,  6.8676e-07,  ..., -3.8634e-08,\n",
            "         2.1911e-04,  1.7681e-02])\n",
            "tensor([ 2.5456e-03,  3.4033e-06,  3.1239e-06,  ..., -5.5899e-08,\n",
            "        -1.3424e-04,  6.0970e-03])\n",
            "tensor([-1.0973e-04,  6.7579e-07,  4.5737e-08,  ..., -7.0676e-09,\n",
            "        -8.7311e-04,  4.5264e-03])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ONgzvHv7oOda",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 5: Vectorizing the text\n",
        "\n",
        "**We’ll be showcasing this using two approaches.**\n",
        "\n",
        " \n",
        "\n",
        "Mean of Word Embeddings within a Tweet\n",
        "\n",
        "We will be calculating the following in this approach:\n",
        "\n",
        "For each sentence:\n",
        "\n",
        "1.   Generate word embedding for each word\n",
        "\n",
        "2.   Calculate the mean of the embeddings of each word to obtain the embedding of the sentence\n"
      ]
    },
    {
      "metadata": {
        "id": "nn-r2-ZymP3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "508ca25c-af47-4db4-e976-bb5cf16c2216"
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm ## tracks progress of loop ##\n",
        "\n",
        "# creating a tensor for storing sentence embeddings #\n",
        "s = torch.zeros(0,z)\n",
        "\n",
        "# iterating Sentence (tqdm tracks progress) #\n",
        "for tweet in tqdm(txt):   \n",
        "  # empty tensor for words #\n",
        "  w = torch.zeros(0,z)   \n",
        "  sentence = Sentence(tweet)\n",
        "  stacked_embeddings.embed(sentence)\n",
        "  # for every word #\n",
        "  for token in sentence:\n",
        "    # storing word Embeddings of each word in a sentence #\n",
        "    w = torch.cat((w,token.embedding.view(-1,z)),0)\n",
        "  # storing sentence Embeddings (mean of embeddings of all words)   #\n",
        "  s = torch.cat((s, w.mean(dim = 0).view(-1, z)),0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▍ | 41613/49159 [48:55<14:29,  8.67it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JMMB8bqHots0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Document Embedding: Vectorizing the entire Tweet\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jY18rAAfosI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import DocumentPoolEmbeddings\n",
        "\n",
        "### initialize the document embeddings, mode = mean ###\n",
        "document_embeddings = DocumentPoolEmbeddings([\n",
        "                                              flair_embedding_backward,\n",
        "                                              flair_embedding_forward\n",
        "                                             ])\n",
        "# Storing Size of embedding #\n",
        "z = sentence.embedding.size()[1]\n",
        "\n",
        "### Vectorising text ###\n",
        "# creating a tensor for storing sentence embeddings\n",
        "s = torch.zeros(0,z)\n",
        "# iterating Sentences #\n",
        "for tweet in tqdm(txt):   \n",
        "  sentence = Sentence(tweet)\n",
        "  document_embeddings.embed(sentence)\n",
        "  # Adding Document embeddings to list #\n",
        "  s = torch.cat((s, sentence.embedding.view(-1,z)),0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVJFdDHXo7hG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can choose either approach for your model. Now that our text is vectorised, we can feed it to our machine learning model!\n"
      ]
    },
    {
      "metadata": {
        "id": "sZxvAQWao9aC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 6: Partitioning the data for Train and Test Sets\n"
      ]
    },
    {
      "metadata": {
        "id": "NjkHPhP2pHKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## tensor to numpy array ##\n",
        "X = s.numpy()   \n",
        "\n",
        "## Test set ##\n",
        "test = X[31962:,:]\n",
        "train = X[:31962,:]\n",
        "\n",
        "# extracting labels of the training set #\n",
        "target = data['label'][data['label'].isnull()==False].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyK_nibMpSp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 6: Building the Model and Defining Custom Evaluator (for F1 Score)\n"
      ]
    },
    {
      "metadata": {
        "id": "mvOzRDX7pN88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining custom F1 evaluator for XGBoost\n",
        "def custom_eval(preds, dtrain):\n",
        "    labels = dtrain.get_label().astype(np.int)\n",
        "    preds = (preds >= 0.3).astype(np.int)\n",
        "    return [('f1_score', f1_score(labels, preds))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4_Dx-i9RpiJ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Building XGBoost Model**"
      ]
    },
    {
      "metadata": {
        "id": "ROn36X9OpeMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "### Splitting training set ###\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train, target,  \n",
        "                                                      random_state=42, \n",
        "                                                          test_size=0.3)\n",
        "\n",
        "### XGBoost compatible data ###\n",
        "dtrain = xgb.DMatrix(x_train,y_train)         \n",
        "dvalid = xgb.DMatrix(x_valid, label = y_valid)\n",
        "\n",
        "### defining parameters ###\n",
        "params = {\n",
        "          'colsample': 0.9,\n",
        "          'colsample_bytree': 0.5,\n",
        "          'eta': 0.1,\n",
        "          'max_depth': 8,\n",
        "          'min_child_weight': 6,\n",
        "          'objective': 'binary:logistic',\n",
        "          'subsample': 0.9\n",
        "          }\n",
        "\n",
        "### Training the model ###\n",
        "xgb_model = xgb.train(\n",
        "                      params,\n",
        "                      dtrain,\n",
        "                      feval= custom_eval,\n",
        "                      num_boost_round= 1000,\n",
        "                      maximize=True,\n",
        "                      evals=[(dvalid, \"Validation\")],\n",
        "                      early_stopping_rounds=30\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6DqbJNWpwe-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 7: Time for predictions!"
      ]
    },
    {
      "metadata": {
        "id": "71umNVuJpska",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "### Reformatting test set for XGB ###\n",
        "dtest = xgb.DMatrix(test)\n",
        "\n",
        "### Predicting ###\n",
        "predict = xgb_model.predict(dtest) # predicting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LlpjlawCp88M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I uploaded the predictions to the practice problem page with 0.2 as probability threshold:\n",
        "\n",
        "Word Embedding\tF1- Score\t\n",
        "Glove\t0.53\t\n",
        "flair-forward -fast\t0.45\t\n",
        "flair-backward-fast\t0.48\t\n",
        "Stacked (flair-forward-fast + flair-backward-fast)\t0.54\t\n",
        " "
      ]
    }
  ]
}